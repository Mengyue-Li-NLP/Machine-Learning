{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgPqt4p1y36p"
      },
      "source": [
        "Evaluating and Extending an RNN based Part-of-Speech Tagging\n",
        "\n",
        "\n",
        "For this assignment, you start with a working pipeline for PoS tagging using LSTMs and mini-batch training. This task is to evaluate the model on a number of sources in different languages, and extend it with new functionality. First, the neural model represents your tokens as non-pretrained embedding vectors. Then the sequences of embedding vectors are passed through an LSTM layer. Finally, the outputs from the recurrent layer are transformed to probabilities over PoS tags by passing them through a fully connected layer and a softmax. You will have to refactor (i.e. rearrange the code) and extend the model by adding some commonly used properties, e.g. bi-directionality (see the list of suggestions below).\n",
        "\n",
        "\n",
        "\n",
        "### Extensions\n",
        "\n",
        "1. There are other types of RNNs layers commonly used in NLP. Add the option to use a GRU layer instead of an LSTM layer, and include this in your performance comparison.\n",
        "2. The given implementation only allows for dependencies from left to right. Add the option to use a bi-directional RNN layer.\n",
        "3. Use pyTorch's `Dataset` and `DataLoader` classes for loading the data. [This tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) is a good starting point. Proper data loaders makes it easier to loop over data sets.\n",
        "4. Try some type of data augmentation in your training data (e.g. masking random tokens). This should theoretically increase the generalizability of your model. How much augmentation is too much?\n",
        "5. Implement some level of regularization in your model. This can be implemented in several ways, e.g. dropout or weight decay. Briefly argue for the choices you made and show how network preformance might change with the rate of regularization.\n",
        "6. Add more sources. Either from different genres and/or adding more languages. A suggestion is to try how languages with very different levels of morphological richness (e.g. English vs Finnish) requires more or less training data, or how well a model trained on academic english does on news text. The total number of sources should be above 10.\n",
        "7. Compare performace using UD's universal vs language specific tag sets.\n",
        "\n",
        "*Note that fully evaluating all combinations of the extensions is not required. It is enough to do some structured testing of extensions and then, for example, go on to compare the tag sets on the best model configuration. This is an exercise in extending and evaluating a model, not in finding the patience to wait for your computer to finish grid searching over alternatives.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDDstzhwztQS"
      },
      "outputs": [],
      "source": [
        "# Our standard imports for maths and basic methodology\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For user feedback\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Imports for pytorch\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okj6Den-4-Wd"
      },
      "source": [
        "Let's see if we have a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYGiUxG746pz",
        "outputId": "2e202677-637e-4eb4-8221-b9de7bbebd6c"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  for i in range(torch.cuda.device_count()):\n",
        "    print(torch.cuda.get_device_name(i))\n",
        "else:\n",
        "  print(\"No GPU available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO0m41-MzJRc"
      },
      "source": [
        "## Load tagging data\n",
        "\n",
        "The following downloads the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus). This data is only here to demonstrate the network below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiiHNHlPSIcD",
        "outputId": "92e73950-5ca4-4946-de91-1eddd87d2864"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def parse_conllu_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    text = response.text\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    words = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith('#'):\n",
        "            continue\n",
        "        columns = line.split('\\t')\n",
        "        if len(columns) > 1:\n",
        "            word = columns[1]\n",
        "            words.append(word)\n",
        "    return words\n",
        "\n",
        "\n",
        "english_url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\"\n",
        "spanish_url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_Spanish-GSD/master/es_gsd-ud-train.conllu\"\n",
        "chinese_url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSDSimp/master/zh_gsdsimp-ud-train.conllu\"\n",
        "german_url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/master/de_gsd-ud-train.conllu\"\n",
        "\n",
        "english_words = parse_conllu_from_url(english_url)\n",
        "spanish_words = parse_conllu_from_url(spanish_url)\n",
        "chinese_words = parse_conllu_from_url(chinese_url)\n",
        "german_words = parse_conllu_from_url(german_url)\n",
        "\n",
        "print(\"English sample:\", english_words[:20])\n",
        "print(\"Spanish sample:\", spanish_words[:20])\n",
        "print(\"Chinese sample:\", chinese_words[:20])\n",
        "print(\"German sample:\", german_words[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l67a4Qnr5ac7",
        "outputId": "29082efb-0f64-4384-87f2-89c3cb8cba8c"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "sentences = brown.tagged_sents(tagset='universal')\n",
        "sentences = [sentence for sentence in sentences if len(sentence) > 2]\n",
        "\n",
        "print(\"Loaded %i sentences\" % len(sentences))\n",
        "print(sentences[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOQVXabf6MH9"
      },
      "source": [
        "Preprocessing for the brow corpus. This splits the data into our standard X and y format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UuA4rJC6Hge",
        "outputId": "0f523a82-94e0-493f-9508-217969cb72a4"
      },
      "outputs": [],
      "source": [
        "X = [[token for token, tag in sentence] for sentence in sentences]\n",
        "y = [[tag for token, tag in sentence] for sentence in sentences]\n",
        "\n",
        "assert len(X) == len(y)\n",
        "\n",
        "print(X[0])\n",
        "print(y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SKxeDs2seHV",
        "outputId": "9a56b935-fa65-4eda-d44f-5d1617b55d86"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
        "\n",
        "assert len(X_train) == len(y_train)\n",
        "assert len(X_test) == len(y_test)\n",
        "assert len(X_train)+len(X_test) == len(X)\n",
        "\n",
        "print(\"The training set includes %i sentences\" % len(X_train))\n",
        "print(\"The test set includes %i sentences\" % len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlL3-sS57GeE"
      },
      "source": [
        "Most sentences are short, but some are very long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "Z25i9_UxtUeR",
        "outputId": "7617e236-2fc4-4c3c-b466-c70bd15e2cee"
      },
      "outputs": [],
      "source": [
        "l = np.asarray([len(x) for x in X], dtype=int)\n",
        "plt.figure(figsize=(8, 4))\n",
        "x = np.unique(l)\n",
        "plt.bar(x, [np.sum(l==e) for e in x], width=1)\n",
        "plt.xlabel(\"Sentence length\")\n",
        "plt.ylabel(\"# sentences\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHIiYxLyz2cf"
      },
      "source": [
        "## Data encoding and padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fjVkyDOP9MU",
        "outputId": "d6908d3f-7390-403c-f672-a276ee775465"
      },
      "outputs": [],
      "source": [
        "tokens = {token for sentence in X_train for token in sentence}\n",
        "idx2token = list(tokens)\n",
        "idx2token.insert(0, '<UNK>')\n",
        "idx2token.append('<PAD>')\n",
        "token2idx = {token:idx for idx, token in enumerate(idx2token)}\n",
        "\n",
        "tags = {tag for tags in y_train for tag in tags}\n",
        "idx2tag = list(tags)\n",
        "idx2tag.append('<PAD>')\n",
        "tag2idx = {tag:idx for idx, tag in enumerate(idx2tag)}\n",
        "\n",
        "print(idx2token[:15])\n",
        "print(idx2tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdgiIDVP9Brw",
        "outputId": "7f8abcb5-b87a-4ddb-bedd-f35095fb99b3"
      },
      "outputs": [],
      "source": [
        "def pad_and_encode(sentences, labels):\n",
        "  assert len(sentences)==len(labels)\n",
        "  assert np.all([len(sentence)==len(tags) for sentence, tags in zip(sentences, labels)])\n",
        "  max_sentence_length = np.max([len(sentence) for sentence in sentences])\n",
        "  padded_sentences = torch.zeros(len(sentences), max_sentence_length,\n",
        "                                 dtype=torch.long)\n",
        "  padded_sentences[:] = token2idx['<PAD>']\n",
        "  padded_labels = torch.zeros(len(sentences), max_sentence_length,\n",
        "                              dtype=torch.long)\n",
        "  padded_labels[:] = tag2idx['<PAD>']\n",
        "  for i, (sentence, tags) in enumerate(zip(sentences, labels)):\n",
        "    for j, token in enumerate(sentence):\n",
        "      if token in token2idx.keys():\n",
        "        padded_sentences[i, j] = token2idx[token]\n",
        "      else:\n",
        "        padded_sentences[i, j] = token2idx['<UNK>']\n",
        "    for j, tag in enumerate(tags):\n",
        "      padded_labels[i, j] = tag2idx[tag]\n",
        "  return padded_sentences, padded_labels\n",
        "\n",
        "a, b = pad_and_encode(X_train[:5], y_train[:5])\n",
        "print(a)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbZWgBRAGWQa",
        "outputId": "8e5211ca-1857-4cad-e437-4f289c83a936"
      },
      "outputs": [],
      "source": [
        "def batch_iterator(sentences, labels, batch_size=64):\n",
        "  assert len(sentences) == len(labels)\n",
        "  for i in range(0, len(sentences), batch_size):\n",
        "    X, y = pad_and_encode(sentences[i:min(i+batch_size, len(sentences))],\n",
        "                          labels[i:min(i+batch_size, len(sentences))])\n",
        "    if torch.cuda.is_available():\n",
        "      yield (X.cuda(), y.cuda())\n",
        "    else:\n",
        "      yield (X, y)\n",
        "\n",
        "next(batch_iterator(X_train, y_train, batch_size=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmgbWbsJk8DW"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwU6cWT2AqT6",
        "outputId": "da617a6a-1091-4231-c5fb-4bca24e2b4de"
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "  def __init__(self, word_embedding_dim, lstm_hidden_dim, vocabulary_size, tagset_size):\n",
        "    super(LSTMTagger, self).__init__()\n",
        "    self.lstm_hidden_dim_ = lstm_hidden_dim\n",
        "    self.vocabulary_size_ = vocabulary_size\n",
        "    self.tagset_size_ = tagset_size\n",
        "\n",
        "    self._word_embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
        "                                         embedding_dim=word_embedding_dim,\n",
        "                                         padding_idx=token2idx['<PAD>'])\n",
        "    self._lstm = nn.LSTM(input_size=word_embedding_dim,\n",
        "                         hidden_size=lstm_hidden_dim,\n",
        "                         batch_first=True)\n",
        "    self._fc = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "    self._softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    self.training_loss_ = list()\n",
        "    self.training_accuracy_ = list()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      self.cuda()\n",
        "\n",
        "  def forward(self, padded_sentences):\n",
        "    \"\"\"The forward pass through the network\"\"\"\n",
        "    batch_size, max_sentence_length = padded_sentences.size()\n",
        "\n",
        "    embedded_sentences = self._word_embedding(padded_sentences)\n",
        "\n",
        "    sentence_lengths = (padded_sentences!=token2idx['<PAD>']).sum(dim=1)\n",
        "    sentence_lengths = sentence_lengths.long().cpu()\n",
        "    X = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentence_lengths,\n",
        "                                          batch_first=True, enforce_sorted=False)\n",
        "    lstm_out, _ = self._lstm(X)\n",
        "    X, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "    X = X.contiguous().view(-1, X.shape[2])\n",
        "    tag_space = self._fc(X)\n",
        "    tag_scores = self._softmax(tag_space)\n",
        "    return tag_scores.view(batch_size, max_sentence_length, self.tagset_size_)\n",
        "\n",
        "\n",
        "model = LSTMTagger(word_embedding_dim=32,\n",
        "                   lstm_hidden_dim=64,\n",
        "                   vocabulary_size=len(token2idx),\n",
        "                   tagset_size=len(tag2idx)-1)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEZlbcallbff"
      },
      "source": [
        "## Network training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVBW4NNNOwY5",
        "outputId": "b642b77b-3397-4715-a048-d1fa22d169f5"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.NLLLoss(ignore_index=tag2idx['<PAD>'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "batch_size = 256\n",
        "for epoch in range(5):\n",
        "  with tqdm(batch_iterator(X_train, y_train, batch_size=batch_size),\n",
        "            total=len(X_train)//batch_size+1, unit=\"batch\", desc=\"Epoch %i\" % epoch) as batches:\n",
        "    for inputs, targets in batches:\n",
        "      model.zero_grad()\n",
        "      scores = model(inputs)\n",
        "      loss = loss_function(scores.view(-1, model.tagset_size_),\n",
        "                           targets.view(-1))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      predictions = scores.argmax(dim=2, keepdim=True).squeeze()\n",
        "      mask = targets!=tag2idx['<PAD>']\n",
        "      correct = (predictions[mask] == targets[mask]).sum().item()\n",
        "      accuracy = correct / mask.sum().item()*100\n",
        "      model.training_accuracy_.append(accuracy)\n",
        "      model.training_loss_.append(loss.item())\n",
        "      batches.set_postfix(loss=loss.item(), accuracy=accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSf4AVZislgg"
      },
      "source": [
        "We can plot the stored loss over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "m6rd8T0_q24D",
        "outputId": "e8b4812d-c9df-4899-bfbd-4f04b49be283"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 4))\n",
        "ax = plt.subplot()\n",
        "ax.set_title(\"Plot of the (hopefully) decreasing loss over epochs\")\n",
        "ax.plot(model.training_loss_, 'b-')\n",
        "ax.set_ylabel(\"Training Loss\", color='b')\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "# ax.set_yscale('log')\n",
        "ax.tick_params(axis='y', labelcolor='b')\n",
        "ax = ax.twinx()\n",
        "ax.plot(model.training_accuracy_, 'r-')\n",
        "ax.set_ylabel(\"Accuracy [%]\", color='r')\n",
        "ax.tick_params(axis='y', labelcolor='r')\n",
        "a = list(ax.axis())\n",
        "a[2] = 0\n",
        "a[3] = 100\n",
        "ax.axis(a)\n",
        "t = np.arange(0, len(model.training_accuracy_), len(X_train)//batch_size+1)\n",
        "ax.set_xticks(ticks=t)\n",
        "ax.set_xticklabels(labels=np.arange(len(t)))\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUDy6CJKcMSh"
      },
      "source": [
        "## Test data accuracy\n",
        "\n",
        "This shows that the model sort of works. A per sentence accuracy would be better while being able to also analyse the predictions qualitatively would be best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48fkSBgEBnFd",
        "outputId": "2a2540f8-3cda-4033-83be-ebc1461ed3d6"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_total = 0\n",
        "  for inputs, targets in batch_iterator(X_test, y_test, batch_size=batch_size):\n",
        "    scores = model(inputs)\n",
        "    predictions = scores.argmax(dim=2, keepdim=True).squeeze()\n",
        "    mask = targets!=tag2idx['<PAD>']\n",
        "    n_correct += (predictions[mask] == targets[mask]).sum().item()\n",
        "    n_total += mask.sum().item()\n",
        "print(\"Test accuracy %.1f%%\" % (100*n_correct/n_total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zogFe6AHN9ge"
      },
      "source": [
        "### Assignment2 postagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoDrr0aJVVVb",
        "outputId": "dd6f2ba5-181d-4f70-e286-c535b3a3a2cd"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install conllu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V5cLk692N9ge",
        "outputId": "1d458eb0-624e-4825-8a4a-faafd9b18b44"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class POSTagger:\n",
        "    def __init__(self, word_embedding_dim=32, lstm_hidden_dim=64, batch_size=256, learning_rate=0.01, epochs=5):\n",
        "        self.word_embedding_dim = word_embedding_dim\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "\n",
        "        self.token2idx = None\n",
        "        self.idx2token = None\n",
        "        self.tag2idx = None\n",
        "        self.idx2tag = None\n",
        "\n",
        "        self.model = None\n",
        "\n",
        "    def load_data(self):\n",
        "        # Load the dataset\n",
        "        dataset = load_dataset(\"universal_dependencies\", \"en_ewt\")\n",
        "\n",
        "        print(dataset)\n",
        "\n",
        "        for sentence in dataset['train']:\n",
        "            tokens = sentence[\"tokens\"]\n",
        "            tags = sentence[\"upos\"]\n",
        "            self.X.append(tokens)\n",
        "            self.y.append(tags)\n",
        "\n",
        "        assert len(self.X) == len(self.y)\n",
        "\n",
        "        print(self.X[0])\n",
        "        print(self.y[0])\n",
        "\n",
        "\n",
        "        sentences = [sentence for sentence in zip(self.X, self.y) if len(sentence[0]) > 2]\n",
        "        self.X = [s[0] for s in sentences]\n",
        "        self.y = [s[1] for s in sentences]\n",
        "\n",
        "        print(\"Loaded %i sentences\" % len(sentences))\n",
        "        print(sentences[0])\n",
        "\n",
        "        #\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=0.1, random_state=0\n",
        "        )\n",
        "\n",
        "        assert len(self.X_train) == len(self.y_train)\n",
        "        assert len(self.X_test) == len(self.y_test)\n",
        "        assert len(self.X_train) + len(self.X_test) == len(self.X)\n",
        "\n",
        "        print(\"The training set includes %i sentences\" % len(self.X_train))\n",
        "        print(\"The test set includes %i sentences\" % len(self.X_test))\n",
        "\n",
        "    def create_vocabulary(self):\n",
        "        tokens = set()\n",
        "        upos = set()\n",
        "\n",
        "\n",
        "        for sentence in self.X_train:\n",
        "            for token in sentence:\n",
        "                tokens.add(token)\n",
        "\n",
        "        for tags in self.y_train:\n",
        "            for tag in tags:\n",
        "                upos.add(tag)\n",
        "\n",
        "        #\n",
        "        self.idx2token = list(tokens)\n",
        "        self.idx2token.insert(0, '<UNK>')\n",
        "        self.idx2token.append('<PAD>')\n",
        "        self.token2idx = {token: idx for idx, token in enumerate(self.idx2token)}\n",
        "\n",
        "        self.idx2tag = list(upos)\n",
        "        self.idx2tag.append('<PAD>')\n",
        "        self.tag2idx = {tag: idx for idx, tag in enumerate(self.idx2tag)}\n",
        "\n",
        "\n",
        "        print(\"Token to Index Mapping:\", self.token2idx)\n",
        "        print(\"Index to Token Mapping:\", self.idx2token)\n",
        "        print(\"Tag to Index Mapping:\", self.tag2idx)\n",
        "        print(\"Index to Tag Mapping:\", self.idx2tag)\n",
        "\n",
        "    def pad_and_encode(self, sentences, labels):\n",
        "        assert len(sentences) == len(labels)\n",
        "        assert np.all([len(sentence) == len(tags) for sentence, tags in zip(sentences, labels)])\n",
        "\n",
        "        max_sentence_length = np.max([len(sentence) for sentence in sentences])\n",
        "        padded_sentences = torch.zeros(len(sentences), max_sentence_length,\n",
        "                                     dtype=torch.long)\n",
        "        padded_sentences[:] = self.token2idx['<PAD>']\n",
        "        padded_labels = torch.zeros(len(sentences), max_sentence_length,\n",
        "                                  dtype=torch.long)\n",
        "        padded_labels[:] = self.tag2idx['<PAD>']\n",
        "\n",
        "        for i, (sentence, tags) in enumerate(zip(sentences, labels)):\n",
        "            for j, token in enumerate(sentence):\n",
        "                if token in self.token2idx.keys():\n",
        "                    padded_sentences[i, j] = self.token2idx[token]\n",
        "                else:\n",
        "                    padded_sentences[i, j] = self.token2idx['<UNK>']\n",
        "            for j, tag in enumerate(tags):\n",
        "                padded_labels[i, j] = self.tag2idx[tag]\n",
        "\n",
        "        return padded_sentences, padded_labels\n",
        "\n",
        "    def batch_iterator(self, sentences, labels, batch_size=64):\n",
        "        \"\"\"Helper function for iterating over batches of the data\"\"\"\n",
        "        assert len(sentences) == len(labels)\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            X, y = self.pad_and_encode(\n",
        "                sentences[i:min(i+batch_size, len(sentences))],\n",
        "                labels[i:min(i+batch_size, len(sentences))]\n",
        "            )\n",
        "            if torch.cuda.is_available():\n",
        "                yield (X.cuda(), y.cuda())\n",
        "            else:\n",
        "                yield (X, y)\n",
        "\n",
        "    def initialize_model(self):\n",
        "        self.model = LSTMTagger(\n",
        "            word_embedding_dim=self.word_embedding_dim,\n",
        "            lstm_hidden_dim=self.lstm_hidden_dim,\n",
        "            vocabulary_size=len(self.token2idx),\n",
        "            tagset_size=len(self.tag2idx)-1,\n",
        "            token2idx=self.token2idx,\n",
        "            tag2idx=self.tag2idx\n",
        "        )\n",
        "        print(self.model)\n",
        "\n",
        "    def train(self):\n",
        "        loss_function = nn.NLLLoss(ignore_index=self.tag2idx['<PAD>'])\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            with tqdm(\n",
        "                self.batch_iterator(self.X_train, self.y_train, batch_size=self.batch_size),\n",
        "                total=len(self.X_train)//self.batch_size+1, unit=\"batch\", desc=\"Epoch %i\" % epoch\n",
        "            ) as batches:\n",
        "                for inputs, targets in batches:\n",
        "                    self.model.zero_grad()\n",
        "                    scores = self.model(inputs)\n",
        "                    loss = loss_function(\n",
        "                        scores.view(-1, self.model.tagset_size_),\n",
        "                        targets.view(-1)\n",
        "                    )\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    predictions = scores.argmax(dim=2, keepdim=True).squeeze()\n",
        "                    mask = targets != self.tag2idx['<PAD>']\n",
        "                    correct = (predictions[mask] == targets[mask]).sum().item()\n",
        "                    accuracy = correct / mask.sum().item() * 100\n",
        "                    self.model.training_accuracy_.append(accuracy)\n",
        "                    self.model.training_loss_.append(loss.item())\n",
        "                    batches.set_postfix(loss=loss.item(), accuracy=accuracy)\n",
        "\n",
        "    def plot_training_progress(self):\n",
        "        fig = plt.figure(figsize=(6, 4))\n",
        "        ax = plt.subplot()\n",
        "        ax.set_title(\"Plot of the (hopefully) decreasing loss over epochs\")\n",
        "        ax.plot(self.model.training_loss_, 'b-')\n",
        "        ax.set_ylabel(\"Training Loss\", color='b')\n",
        "        ax.set_xlabel(\"Epoch\")\n",
        "        ax.tick_params(axis='y', labelcolor='b')\n",
        "        ax = ax.twinx()\n",
        "        ax.plot(self.model.training_accuracy_, 'r-')\n",
        "        ax.set_ylabel(\"Accuracy [%]\", color='r')\n",
        "        ax.tick_params(axis='y', labelcolor='r')\n",
        "        a = list(ax.axis())\n",
        "        a[2] = 0\n",
        "        a[3] = 100\n",
        "        ax.axis(a)\n",
        "        t = np.arange(0, len(self.model.training_accuracy_), len(self.X_train)//self.batch_size+1)\n",
        "        ax.set_xticks(ticks=t)\n",
        "        ax.set_xticklabels(labels=np.arange(len(t)))\n",
        "        fig.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate(self):\n",
        "        with torch.no_grad():\n",
        "            n_correct = 0\n",
        "            n_total = 0\n",
        "            for inputs, targets in self.batch_iterator(self.X_test, self.y_test, batch_size=self.batch_size):\n",
        "                scores = self.model(inputs)\n",
        "                predictions = scores.argmax(dim=2, keepdim=True).squeeze()\n",
        "                mask = targets != self.tag2idx['<PAD>']\n",
        "                n_correct += (predictions[mask] == targets[mask]).sum().item()\n",
        "                n_total += mask.sum().item()\n",
        "        print(\"Test accuracy %.1f%%\" % (100*n_correct/n_total))\n",
        "\n",
        "    def run(self):\n",
        "        self.load_data()\n",
        "        self.create_vocabulary()\n",
        "        self.initialize_model()\n",
        "        self.train()\n",
        "        self.plot_training_progress()\n",
        "        self.evaluate()\n",
        "\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, word_embedding_dim, lstm_hidden_dim, vocabulary_size, tagset_size, token2idx, tag2idx):\n",
        "\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.lstm_hidden_dim_ = lstm_hidden_dim\n",
        "        self.vocabulary_size_ = vocabulary_size\n",
        "        self.tagset_size_ = tagset_size\n",
        "        self.token2idx = token2idx\n",
        "\n",
        "        self._word_embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
        "                                           embedding_dim=word_embedding_dim,\n",
        "                                           padding_idx=token2idx['<PAD>'])\n",
        "        self._lstm = nn.LSTM(input_size=word_embedding_dim,\n",
        "                           hidden_size=lstm_hidden_dim,\n",
        "                           batch_first=True)\n",
        "        self._fc = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "        self._softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        self.training_loss_ = list()\n",
        "        self.training_accuracy_ = list()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "\n",
        "    def forward(self, padded_sentences):\n",
        "        \"\"\"The forward pass through the network\"\"\"\n",
        "        batch_size, max_sentence_length = padded_sentences.size()\n",
        "\n",
        "        embedded_sentences = self._word_embedding(padded_sentences)\n",
        "\n",
        "        sentence_lengths = (padded_sentences!=self.token2idx['<PAD>']).sum(dim=1)\n",
        "        sentence_lengths = sentence_lengths.long().cpu()\n",
        "        X = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentence_lengths,\n",
        "                                            batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, _ = self._lstm(X)\n",
        "        X, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "        X = X.contiguous().view(-1, X.shape[2])\n",
        "        tag_space = self._fc(X)\n",
        "        tag_scores = self._softmax(tag_space)\n",
        "        return tag_scores.view(batch_size, max_sentence_length, self.tagset_size_)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    pos_tagger = POSTagger(\n",
        "        word_embedding_dim=32,\n",
        "        lstm_hidden_dim=64,\n",
        "        batch_size=256,\n",
        "        learning_rate=0.01,\n",
        "        epochs=5\n",
        "    )\n",
        "    pos_tagger.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grcjtI0FN9gf"
      },
      "source": [
        "###  Use a GRU layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3sV1W7UfN9gf",
        "outputId": "57305c17-a58f-4be6-d49e-333643f18080"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class POSTagger:\n",
        "    def __init__(self, word_embedding_dim=32, hidden_dim=64, batch_size=256, learning_rate=0.01, epochs=5, rnn_type='lstm'):\n",
        "        self.word_embedding_dim = word_embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "\n",
        "        if self.rnn_type not in ['lstm', 'gru']:\n",
        "            raise ValueError(\"rnn_type must be either 'lstm' or 'gru'\")\n",
        "\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "\n",
        "        self.token2idx = None\n",
        "        self.idx2token = None\n",
        "        self.tag2idx = None\n",
        "        self.idx2tag = None\n",
        "\n",
        "        self.model = None\n",
        "\n",
        "    def compute_baseline_accuracy(self):\n",
        "        from collections import defaultdict, Counter\n",
        "\n",
        "        token2tags = defaultdict(Counter)\n",
        "        for sentence, tags in zip(self.X_train, self.y_train):\n",
        "            for tk, tg in zip(sentence, tags):\n",
        "                token2tags[tk][tg] += 1\n",
        "\n",
        "        token2best = {}\n",
        "        global_tag_counter = Counter()\n",
        "        for tk, c in token2tags.items():\n",
        "          best_tag, _ = c.most_common(1)[0]\n",
        "          token2best[tk] = best_tag\n",
        "          global_tag_counter.update(c.keys())\n",
        "        global_most_common_tag, _ = global_tag_counter.most_common(1)[0]\n",
        "\n",
        "\n",
        "        n_total = 0\n",
        "        n_correct = 0\n",
        "        for sentence, tags in zip(self.X_test, self.y_test):\n",
        "          for tk, tg in zip(sentence, tags):\n",
        "            if tk == \"<PAD>\":\n",
        "                continue\n",
        "            if tk in token2best:\n",
        "                pred = token2best[tk]\n",
        "            else:\n",
        "                pred = global_most_common_tag\n",
        "            if pred == tg:\n",
        "                n_correct += 1\n",
        "            n_total += 1\n",
        "\n",
        "        baseline_acc = 100.0 * n_correct / n_total if n_total > 0 else 0.0\n",
        "        print(f\"Baseline accuracy: {baseline_acc:.1f}%\")\n",
        "        return baseline_acc\n",
        "\n",
        "\n",
        "    def load_data(self, language=\"en_ewt\"):\n",
        "        print(f\"Loading dataset [universal_dependencies] with config = '{language}'\")\n",
        "        dataset = load_dataset(\"universal_dependencies\", language)\n",
        "\n",
        "        print(dataset)\n",
        "\n",
        "        for sentence in dataset['train']:\n",
        "            tokens = sentence[\"tokens\"]\n",
        "            tags = sentence[\"upos\"]\n",
        "            self.X.append(tokens)\n",
        "            self.y.append(tags)\n",
        "\n",
        "        assert len(self.X) == len(self.y)\n",
        "\n",
        "        print(self.X[0])\n",
        "        print(self.y[0])\n",
        "\n",
        "\n",
        "        sentences = [sentence for sentence in zip(self.X, self.y) if len(sentence[0]) > 2]\n",
        "        self.X = [s[0] for s in sentences]\n",
        "        self.y = [s[1] for s in sentences]\n",
        "\n",
        "        print(\"Loaded %i sentences\" % len(sentences))\n",
        "        print(sentences[0])\n",
        "\n",
        "\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=0.1, random_state=0\n",
        "        )\n",
        "\n",
        "        assert len(self.X_train) == len(self.y_train)\n",
        "        assert len(self.X_test) == len(self.y_test)\n",
        "        assert len(self.X_train) + len(self.X_test) == len(self.X)\n",
        "\n",
        "        print(\"The training set includes %i sentences\" % len(self.X_train))\n",
        "        print(\"The test set includes %i sentences\" % len(self.X_test))\n",
        "\n",
        "    def create_vocabulary(self):\n",
        "\n",
        "        tokens = set()\n",
        "        upos = set()\n",
        "\n",
        "\n",
        "        for sentence in self.X_train:\n",
        "            for token in sentence:\n",
        "                tokens.add(token)\n",
        "\n",
        "        for tags in self.y_train:\n",
        "            for tag in tags:\n",
        "                upos.add(tag)\n",
        "\n",
        "\n",
        "        self.idx2token = list(tokens)\n",
        "        self.idx2token.insert(0, '<UNK>')\n",
        "        self.idx2token.append('<PAD>')\n",
        "        self.token2idx = {token: idx for idx, token in enumerate(self.idx2token)}\n",
        "\n",
        "        self.idx2tag = list(upos)\n",
        "        self.idx2tag.append('<PAD>')\n",
        "        self.tag2idx = {tag: idx for idx, tag in enumerate(self.idx2tag)}\n",
        "\n",
        "\n",
        "        print(\"Token to Index Mapping:\", self.token2idx)\n",
        "        print(\"Index to Token Mapping:\", self.idx2token)\n",
        "        print(\"Tag to Index Mapping:\", self.tag2idx)\n",
        "        print(\"Index to Tag Mapping:\", self.idx2tag)\n",
        "\n",
        "    def pad_and_encode(self, sentences, labels):\n",
        "      assert all(len(s) == len(l) for s, l in zip(sentences, labels))\n",
        "      max_sentence_length = np.max([len(sentence) for sentence in sentences])\n",
        "\n",
        "      padded_sentences = torch.zeros(len(sentences), max_sentence_length, dtype=torch.long)   # Use dict.get(token, token2idx[\"<UNK>\"]) to handle out-of-vocabulary (OOV) tokens.\n",
        "                                                                                              # If a token is not in the vocabulary, it automatically defaults to the <UNK> index.\n",
        "      padded_labels = torch.zeros(len(sentences), max_sentence_length, dtype=torch.long)\n",
        "\n",
        "      for i, (sentence, tags) in enumerate(zip(sentences, labels)):\n",
        "        for j, token in enumerate(sentence):\n",
        "            padded_sentences[i, j] = self.token2idx.get(token, self.token2idx[\"<UNK>\"])\n",
        "\n",
        "        for j, tag in enumerate(tags):\n",
        "            padded_labels[i, j] = self.tag2idx[tag]\n",
        "\n",
        "      return padded_sentences, padded_labels\n",
        "\n",
        "\n",
        "    def batch_iterator(self, sentences, labels, batch_size=64):\n",
        "        assert len(sentences) == len(labels)\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            X, y = self.pad_and_encode(\n",
        "                sentences[i:min(i+batch_size, len(sentences))],\n",
        "                labels[i:min(i+batch_size, len(sentences))]\n",
        "            )\n",
        "            if torch.cuda.is_available():\n",
        "                yield (X.cuda(), y.cuda())\n",
        "            else:\n",
        "                yield (X, y)\n",
        "\n",
        "    def initialize_model(self):\n",
        "        self.model = RNNTagger(\n",
        "            word_embedding_dim=self.word_embedding_dim,\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            vocabulary_size=len(self.token2idx),\n",
        "            tagset_size=len(self.tag2idx)-1,\n",
        "            token2idx=self.token2idx,\n",
        "            tag2idx=self.tag2idx,\n",
        "            rnn_type=self.rnn_type\n",
        "        )\n",
        "        print(self.model)\n",
        "\n",
        "    def train(self):\n",
        "        loss_function = nn.NLLLoss(ignore_index=self.tag2idx['<PAD>'])\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            with tqdm(\n",
        "                self.batch_iterator(self.X_train, self.y_train, batch_size=self.batch_size),\n",
        "                total=len(self.X_train)//self.batch_size+1, unit=\"batch\", desc=f\"Epoch {epoch} ({self.rnn_type.upper()})\"\n",
        "            ) as batches:\n",
        "                for inputs, targets in batches:\n",
        "                    self.model.zero_grad()\n",
        "                    scores = self.model(inputs)\n",
        "                    loss = loss_function(\n",
        "                        scores.view(-1, self.model.tagset_size_),\n",
        "                        targets.view(-1)\n",
        "                    )\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    predictions = scores.argmax(dim=2, keepdim=True).squeeze()\n",
        "                    mask = targets != self.tag2idx['<PAD>']\n",
        "                    correct = (predictions[mask] == targets[mask]).sum().item()\n",
        "                    accuracy = correct / mask.sum().item() * 100\n",
        "                    self.model.training_accuracy_.append(accuracy)\n",
        "                    self.model.training_loss_.append(loss.item())\n",
        "                    batches.set_postfix(loss=loss.item(), accuracy=accuracy)\n",
        "\n",
        "    def plot_training_progress(self):\n",
        "        fig = plt.figure(figsize=(6, 4))\n",
        "        ax = plt.subplot()\n",
        "        ax.set_title(f\"Training Progress with {self.rnn_type.upper()}\")\n",
        "        ax.plot(self.model.training_loss_, 'b-')\n",
        "        ax.set_ylabel(\"Training Loss\", color='b')\n",
        "        ax.set_xlabel(\"Epoch\")\n",
        "        ax.tick_params(axis='y', labelcolor='b')\n",
        "        ax = ax.twinx()\n",
        "        ax.plot(self.model.training_accuracy_, 'r-')\n",
        "        ax.set_ylabel(\"Accuracy [%]\", color='r')\n",
        "        ax.tick_params(axis='y', labelcolor='r')\n",
        "        a = list(ax.axis())\n",
        "        a[2] = 0\n",
        "        a[3] = 100\n",
        "        ax.axis(a)\n",
        "        t = np.arange(0, len(self.model.training_accuracy_), len(self.X_train)//self.batch_size+1)\n",
        "        ax.set_xticks(ticks=t)\n",
        "        ax.set_xticklabels(labels=np.arange(len(t)))\n",
        "        fig.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate(self):\n",
        "        baseline_acc = self.compute_baseline_accuracy()\n",
        "        with torch.no_grad():\n",
        "            n_correct = 0\n",
        "            n_total = 0\n",
        "            for inputs, targets in self.batch_iterator(self.X_test, self.y_test, batch_size=self.batch_size):\n",
        "                scores = self.model(inputs)\n",
        "                predictions = scores.argmax(dim=2, keepdim=True).squeeze()\n",
        "                mask = targets != self.tag2idx['<PAD>']\n",
        "                n_correct += (predictions[mask] == targets[mask]).sum().item()\n",
        "                n_total += mask.sum().item()\n",
        "        print(f\"Test accuracy with {self.rnn_type.upper()}: {100*n_correct/n_total:.1f}%\")\n",
        "        print(f\"Baseline Accuracy is: {baseline_acc:.1f}%\")\n",
        "\n",
        "    def run(self, language=\"en_ewt\"):\n",
        "        self.load_data(language=language)\n",
        "        self.create_vocabulary()\n",
        "        self.initialize_model()\n",
        "        self.train()\n",
        "        self.plot_training_progress()\n",
        "        self.evaluate()\n",
        "\n",
        "\n",
        "class RNNTagger(nn.Module):\n",
        "    def __init__(self, word_embedding_dim, hidden_dim, vocabulary_size, tagset_size, token2idx, tag2idx, rnn_type='lstm'):\n",
        "        super(RNNTagger, self).__init__()\n",
        "        self.hidden_dim_ = hidden_dim\n",
        "        self.vocabulary_size_ = vocabulary_size\n",
        "        self.tagset_size_ = tagset_size\n",
        "        self.token2idx = token2idx\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "\n",
        "        self._word_embedding = nn.Embedding(\n",
        "            num_embeddings=vocabulary_size,\n",
        "            embedding_dim=word_embedding_dim,\n",
        "            padding_idx=token2idx['<PAD>']\n",
        "        )\n",
        "\n",
        "\n",
        "        if self.rnn_type == 'lstm':\n",
        "            self._rnn = nn.LSTM(\n",
        "                input_size=word_embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                batch_first=True\n",
        "            )\n",
        "        elif self.rnn_type == 'gru':\n",
        "            self._rnn = nn.GRU(\n",
        "                input_size=word_embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                batch_first=True\n",
        "            )\n",
        "\n",
        "        self._fc = nn.Linear(hidden_dim, tagset_size)\n",
        "        self._softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        self.training_loss_ = list()\n",
        "        self.training_accuracy_ = list()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "\n",
        "    def forward(self, padded_sentences):\n",
        "        \"\"\"The forward pass through the network\"\"\"\n",
        "        batch_size, max_sentence_length = padded_sentences.size()\n",
        "\n",
        "        embedded_sentences = self._word_embedding(padded_sentences)\n",
        "\n",
        "        sentence_lengths = (padded_sentences != self.token2idx['<PAD>']).sum(dim=1)\n",
        "        sentence_lengths = sentence_lengths.long().cpu()\n",
        "\n",
        "        X = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded_sentences,\n",
        "            sentence_lengths,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        rnn_out, _ = self._rnn(X)\n",
        "        X, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
        "\n",
        "        X = X.contiguous().view(-1, X.shape[2])\n",
        "        tag_space = self._fc(X)\n",
        "        tag_scores = self._softmax(tag_space)\n",
        "\n",
        "        return tag_scores.view(batch_size, max_sentence_length, self.tagset_size_)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rnn_type = 'gru'\n",
        "\n",
        "    languages = [\"en_ewt\", \"es_gsd\", \"zh_gsdsimp\", \"de_gsd\"]\n",
        "    for lang in languages:\n",
        "        print(f\"=== Extension1: RNN={rnn_type}, Language={lang} ===\")\n",
        "        pos_tagger = POSTagger(\n",
        "            word_embedding_dim=32,\n",
        "            hidden_dim=64,\n",
        "            batch_size=256,\n",
        "            learning_rate=0.01,\n",
        "            epochs=5,\n",
        "            rnn_type=rnn_type\n",
        "        )\n",
        "        pos_tagger.run(language=lang)\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R948diOffYRP"
      },
      "source": [
        "In these training curves and test results, the models using GRU layers exhibit a good convergence trend across four languages (English, Spanish, Chinese, and German). As the training epochs progress, the training loss drops rapidly, while the accuracy curve rises significantly.\n",
        "\n",
        "This variation in accuracy may be related to differences in language structure and corpus size. Spanish and English have relatively clear morphological variations and syntactic rules, making it easier for the model to learn stable part-of-speech features, leading to higher accuracy. In contrast, Chinese lacks morphological markers and requires word segmentation, which means the model needs deeper contextual information to accurately distinguish part-of-speech categories, resulting in slightly lower accuracy. Although German belongs to the same Indo-European language family as English, its complex inflectional morphology poses higher demands on the memory and generalization capabilities of RNN models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kluEpHdBN9gf"
      },
      "source": [
        " ### Use a bi-directional RNN layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p8eCMQ2NN9gf",
        "outputId": "a4f06ae6-d90f-4ec2-d483-ffa28d038d72"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class POSTagger:\n",
        "    def __init__(self, word_embedding_dim=32, hidden_dim=64, batch_size=256, learning_rate=0.01, epochs=5, rnn_type='lstm', bidirectional=True):\n",
        "        self.word_embedding_dim = word_embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "        if self.rnn_type not in ['lstm', 'gru']:\n",
        "            raise ValueError(\"rnn_type must be either 'lstm' or 'gru'\")\n",
        "\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "\n",
        "        self.token2idx = None\n",
        "        self.idx2token = None\n",
        "        self.tag2idx = None\n",
        "        self.idx2tag = None\n",
        "\n",
        "        self.model = None\n",
        "\n",
        "    def compute_baseline_accuracy(self):\n",
        "        from collections import defaultdict, Counter\n",
        "        token2tags = defaultdict(Counter)\n",
        "        for sentence, tags in zip(self.X_train, self.y_train):\n",
        "            for tk, tg in zip(sentence, tags):\n",
        "                token2tags[tk][tg] += 1\n",
        "\n",
        "        token2best = {}\n",
        "        global_tag_counter = Counter()\n",
        "        for tk, c in token2tags.items():\n",
        "          best_tag, _ = c.most_common(1)[0]\n",
        "          token2best[tk] = best_tag\n",
        "          global_tag_counter.update(c.keys())\n",
        "\n",
        "        global_most_common_tag, _ = global_tag_counter.most_common(1)[0]\n",
        "\n",
        "        n_total = 0\n",
        "        n_correct = 0\n",
        "        for sentence, tags in zip(self.X_test, self.y_test):\n",
        "          for tk, tg in zip(sentence, tags):\n",
        "            if tk == \"<PAD>\":\n",
        "                continue\n",
        "            if tk in token2best:\n",
        "                pred = token2best[tk]\n",
        "            else:\n",
        "                pred = global_most_common_tag\n",
        "            if pred == tg:\n",
        "                n_correct += 1\n",
        "            n_total += 1\n",
        "\n",
        "        baseline_acc = 100.0 * n_correct / n_total if n_total > 0 else 0.0\n",
        "        print(f\"Baseline accuracy: {baseline_acc:.1f}%\")\n",
        "        return baseline_acc\n",
        "\n",
        "    def load_data(self, language=\"en_ewt\"):\n",
        "        print(f\"Loading dataset [universal_dependencies] with config = '{language}'\")\n",
        "        dataset = load_dataset(\"universal_dependencies\", language)\n",
        "\n",
        "        print(dataset)\n",
        "\n",
        "        for sentence in dataset['train']:\n",
        "            tokens = sentence[\"tokens\"]\n",
        "            tags = sentence[\"upos\"]\n",
        "            self.X.append(tokens)\n",
        "            self.y.append(tags)\n",
        "\n",
        "        assert len(self.X) == len(self.y)\n",
        "\n",
        "        print(self.X[0])\n",
        "        print(self.y[0])\n",
        "\n",
        "\n",
        "        sentences = [sentence for sentence in zip(self.X, self.y) if len(sentence[0]) > 2]\n",
        "        self.X = [s[0] for s in sentences]\n",
        "        self.y = [s[1] for s in sentences]\n",
        "\n",
        "        print(\"Loaded %i sentences\" % len(sentences))\n",
        "        print(sentences[0])\n",
        "\n",
        "\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=0.1, random_state=0\n",
        "        )\n",
        "\n",
        "        assert len(self.X_train) == len(self.y_train)\n",
        "        assert len(self.X_test) == len(self.y_test)\n",
        "        assert len(self.X_train) + len(self.X_test) == len(self.X)\n",
        "\n",
        "        print(\"The training set includes %i sentences\" % len(self.X_train))\n",
        "        print(\"The test set includes %i sentences\" % len(self.X_test))\n",
        "\n",
        "    def create_vocabulary(self):\n",
        "\n",
        "        tokens = set()\n",
        "        upos = set()\n",
        "\n",
        "\n",
        "        for sentence in self.X_train:\n",
        "            for token in sentence:\n",
        "                tokens.add(token)\n",
        "\n",
        "        for tags in self.y_train:\n",
        "            for tag in tags:\n",
        "                upos.add(tag)\n",
        "\n",
        "\n",
        "        self.idx2token = list(tokens)\n",
        "        self.idx2token.insert(0, '<UNK>')\n",
        "        self.idx2token.append('<PAD>')\n",
        "        self.token2idx = {token: idx for idx, token in enumerate(self.idx2token)}\n",
        "\n",
        "        self.idx2tag = list(upos)\n",
        "        self.idx2tag.append('<PAD>')\n",
        "        self.tag2idx = {tag: idx for idx, tag in enumerate(self.idx2tag)}\n",
        "\n",
        "\n",
        "        print(\"Token to Index Mapping:\", self.token2idx)\n",
        "        print(\"Index to Token Mapping:\", self.idx2token)\n",
        "        print(\"Tag to Index Mapping:\", self.tag2idx)\n",
        "        print(\"Index to Tag Mapping:\", self.idx2tag)\n",
        "\n",
        "    def pad_and_encode(self, sentences, labels):\n",
        "      assert all(len(s) == len(l) for s, l in zip(sentences, labels))\n",
        "      max_sentence_length = np.max([len(sentence) for sentence in sentences])\n",
        "\n",
        "      padded_sentences = torch.zeros(len(sentences), max_sentence_length, dtype=torch.long)\n",
        "      padded_labels = torch.zeros(len(sentences), max_sentence_length, dtype=torch.long)      # Use dict.get(token, token2idx[\"<UNK>\"]) to handle out-of-vocabulary (OOV) tokens.\n",
        "                                                                                              # If a token is not in the vocabulary, it automatically defaults to the <UNK> index.\n",
        "\n",
        "      for i, (sentence, tags) in enumerate(zip(sentences, labels)):\n",
        "        for j, token in enumerate(sentence):\n",
        "            padded_sentences[i, j] = self.token2idx.get(token, self.token2idx[\"<UNK>\"])\n",
        "\n",
        "        for j, tag in enumerate(tags):\n",
        "            padded_labels[i, j] = self.tag2idx[tag]\n",
        "\n",
        "      return padded_sentences, padded_labels\n",
        "\n",
        "    def batch_iterator(self, sentences, labels, batch_size=64):\n",
        "        assert len(sentences) == len(labels)\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            X, y = self.pad_and_encode(\n",
        "                sentences[i:min(i+batch_size, len(sentences))],\n",
        "                labels[i:min(i+batch_size, len(sentences))]\n",
        "            )\n",
        "            if torch.cuda.is_available():\n",
        "                yield (X.cuda(), y.cuda())\n",
        "            else:\n",
        "                yield (X, y)\n",
        "\n",
        "    def initialize_model(self):\n",
        "        self.model = RNNTagger(\n",
        "            word_embedding_dim=self.word_embedding_dim,\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            vocabulary_size=len(self.token2idx),\n",
        "            tagset_size=len(self.tag2idx)-1,\n",
        "            token2idx=self.token2idx,\n",
        "            tag2idx=self.tag2idx,\n",
        "            rnn_type=self.rnn_type,\n",
        "            bidirectional=self.bidirectional\n",
        "        )\n",
        "        print(self.model)\n",
        "\n",
        "    def train(self):\n",
        "        loss_function = nn.NLLLoss(ignore_index=self.tag2idx['<PAD>'])\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            with tqdm(\n",
        "                self.batch_iterator(self.X_train, self.y_train, batch_size=self.batch_size),\n",
        "                total=len(self.X_train)//self.batch_size+1, unit=\"batch\",\n",
        "                desc=f\"Epoch {epoch} ({self.rnn_type.upper()}, {'Bi' if self.bidirectional else 'Uni'}directional)\"\n",
        "            ) as batches:\n",
        "                for inputs, targets in batches:\n",
        "                    self.model.zero_grad()\n",
        "                    scores = self.model(inputs)\n",
        "                    loss = loss_function(\n",
        "                        scores.view(-1, self.model.tagset_size_),\n",
        "                        targets.view(-1)\n",
        "                    )\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    predictions = scores.argmax(dim=2, keepdim=True).squeeze()\n",
        "                    mask = targets != self.tag2idx['<PAD>']\n",
        "                    correct = (predictions[mask] == targets[mask]).sum().item()\n",
        "                    accuracy = correct / mask.sum().item() * 100\n",
        "                    self.model.training_accuracy_.append(accuracy)\n",
        "                    self.model.training_loss_.append(loss.item())\n",
        "                    batches.set_postfix(loss=loss.item(), accuracy=accuracy)\n",
        "\n",
        "    def plot_training_progress(self):\n",
        "        fig = plt.figure(figsize=(6, 4))\n",
        "        ax = plt.subplot()\n",
        "        direction = 'Bidirectional' if self.bidirectional else 'Unidirectional'\n",
        "        ax.set_title(f\"Training Progress with {self.rnn_type.upper()} ({direction})\")\n",
        "        ax.plot(self.model.training_loss_, 'b-')\n",
        "        ax.set_ylabel(\"Training Loss\", color='b')\n",
        "        ax.set_xlabel(\"Epoch\")\n",
        "        ax.tick_params(axis='y', labelcolor='b')\n",
        "        ax = ax.twinx()\n",
        "        ax.plot(self.model.training_accuracy_, 'r-')\n",
        "        ax.set_ylabel(\"Accuracy [%]\", color='r')\n",
        "        ax.tick_params(axis='y', labelcolor='r')\n",
        "        a = list(ax.axis())\n",
        "        a[2] = 0\n",
        "        a[3] = 100\n",
        "        ax.axis(a)\n",
        "        t = np.arange(0, len(self.model.training_accuracy_), len(self.X_train)//self.batch_size+1)\n",
        "        ax.set_xticks(ticks=t)\n",
        "        ax.set_xticklabels(labels=np.arange(len(t)))\n",
        "        fig.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate(self):\n",
        "        baseline_acc = self.compute_baseline_accuracy()\n",
        "        with torch.no_grad():\n",
        "            n_correct = 0\n",
        "            n_total = 0\n",
        "            for inputs, targets in self.batch_iterator(self.X_test, self.y_test, batch_size=self.batch_size):\n",
        "                scores = self.model(inputs)\n",
        "                predictions = scores.argmax(dim=2, keepdim=True).squeeze()\n",
        "                mask = targets != self.tag2idx['<PAD>']\n",
        "                n_correct += (predictions[mask] == targets[mask]).sum().item()\n",
        "                n_total += mask.sum().item()\n",
        "\n",
        "        direction = 'Bidirectional' if self.bidirectional else 'Unidirectional'\n",
        "        print(f\"Test accuracy with {self.rnn_type.upper()} ({direction}): {100*n_correct/n_total:.1f}%\")\n",
        "        print(f\"Baseline Accuracy is: {baseline_acc:.1f}%\")\n",
        "\n",
        "    def run(self, language=\"en_ewt\"):\n",
        "        self.load_data(language=language)\n",
        "        self.create_vocabulary()\n",
        "        self.initialize_model()\n",
        "        self.train()\n",
        "        self.plot_training_progress()\n",
        "        self.evaluate()\n",
        "\n",
        "\n",
        "class RNNTagger(nn.Module):\n",
        "    def __init__(self, word_embedding_dim, hidden_dim, vocabulary_size, tagset_size, token2idx, tag2idx, rnn_type='lstm', bidirectional=True):\n",
        "        super(RNNTagger, self).__init__()\n",
        "        self.hidden_dim_ = hidden_dim\n",
        "        self.vocabulary_size_ = vocabulary_size\n",
        "        self.tagset_size_ = tagset_size\n",
        "        self.token2idx = token2idx\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self._word_embedding = nn.Embedding(\n",
        "            num_embeddings=vocabulary_size,\n",
        "            embedding_dim=word_embedding_dim,\n",
        "            padding_idx=token2idx['<PAD>']\n",
        "        )\n",
        "\n",
        "\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "\n",
        "        if self.rnn_type == 'lstm':\n",
        "            self._rnn = nn.LSTM(\n",
        "                input_size=word_embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                batch_first=True,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "        elif self.rnn_type == 'gru':\n",
        "            self._rnn = nn.GRU(\n",
        "                input_size=word_embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                batch_first=True,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "\n",
        "\n",
        "        rnn_output_dim = hidden_dim * self.num_directions\n",
        "\n",
        "        self._fc = nn.Linear(rnn_output_dim, tagset_size)\n",
        "        self._softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        self.training_loss_ = list()\n",
        "        self.training_accuracy_ = list()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "\n",
        "    def forward(self, padded_sentences):\n",
        "        batch_size, max_sentence_length = padded_sentences.size()\n",
        "\n",
        "        embedded_sentences = self._word_embedding(padded_sentences)\n",
        "\n",
        "        sentence_lengths = (padded_sentences != self.token2idx['<PAD>']).sum(dim=1)\n",
        "        sentence_lengths = sentence_lengths.long().cpu()\n",
        "\n",
        "        X = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded_sentences,\n",
        "            sentence_lengths,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        rnn_out, _ = self._rnn(X)\n",
        "        X, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
        "\n",
        "        X = X.contiguous().view(-1, X.shape[2])\n",
        "        tag_space = self._fc(X)\n",
        "        tag_scores = self._softmax(tag_space)\n",
        "\n",
        "        return tag_scores.view(batch_size, max_sentence_length, self.tagset_size_)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rnn_type = 'lstm'\n",
        "    bidirectional = True\n",
        "\n",
        "    languages = [\"en_ewt\", \"es_gsd\", \"zh_gsdsimp\", \"de_gsd\"]\n",
        "    for lang in languages:\n",
        "        print(f\"=== Extension1: RNN={rnn_type}, Language={lang} ===\")\n",
        "        pos_tagger = POSTagger(\n",
        "            word_embedding_dim=32,\n",
        "            hidden_dim=64,\n",
        "            batch_size=256,\n",
        "            learning_rate=0.01,\n",
        "            epochs=5,\n",
        "            rnn_type=rnn_type\n",
        "        )\n",
        "        pos_tagger.run(language=lang)\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFEPc0cipptp"
      },
      "source": [
        "In this experiment, we used a Bidirectional LSTM (BiLSTM) model for part-of-speech tagging on English, Spanish, Chinese, and German. It can be observed that English and Spanish achieve relatively higher accuracy, which may be attributed to their more regular morphological variations and word order characteristics, making it easier for the model to extract relevant features. German also achieves a good performance, but due to its complex inflectional morphology, the model requires stronger capabilities to capture case inflections and word order information.\n",
        "\n",
        "The accuracy for Chinese is noticeably lower, indicating that despite BiLSTM’s ability to capture bidirectional context, it still struggles with Chinese features, particularly in the absence of explicit morphological markers and the ambiguity introduced by word segmentation. Additionally, differences in dataset sizes and annotation standards across languages also affect the model’s learning performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40MNU2D3N9gf"
      },
      "source": [
        "### Use pyTorch's Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NfvW1Q07N9gf",
        "outputId": "ea0ce616-d7f2-4662-b84f-54de9270232e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, token2idx, tag2idx):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.token2idx = token2idx\n",
        "        self.tag2idx = tag2idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "\n",
        "        token_indices = []\n",
        "        for token in sentence:\n",
        "            if token in self.token2idx:\n",
        "                token_indices.append(self.token2idx[token])\n",
        "            else:\n",
        "                token_indices.append(self.token2idx['<UNK>'])\n",
        "\n",
        "\n",
        "        tag_indices = [self.tag2idx[tag] for tag in tag_seq]\n",
        "\n",
        "        return {\n",
        "            'sentence': sentence,\n",
        "            'tag_seq': tag_seq,\n",
        "            'token_indices': token_indices,\n",
        "            'tag_indices': tag_indices,\n",
        "            'length': len(sentence)\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = sorted(batch, key=lambda x: x['length'], reverse=True)\n",
        "\n",
        "    max_len = max([item['length'] for item in batch])\n",
        "\n",
        "    batch_size = len(batch)\n",
        "    token_indices_batch = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
        "    tag_indices_batch = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
        "    lengths = torch.LongTensor([item['length'] for item in batch])\n",
        "\n",
        "    for i, item in enumerate(batch):\n",
        "        tokens = torch.tensor(item['token_indices'], dtype=torch.long)\n",
        "        tags = torch.tensor(item['tag_indices'], dtype=torch.long)\n",
        "        token_indices_batch[i, :len(tokens)] = tokens\n",
        "        tag_indices_batch[i, :len(tags)] = tags\n",
        "\n",
        "    return {\n",
        "        'token_indices': token_indices_batch,\n",
        "        'tag_indices': tag_indices_batch,\n",
        "        'lengths': lengths,\n",
        "        'sentences': [item['sentence'] for item in batch],\n",
        "        'tag_seqs': [item['tag_seq'] for item in batch]\n",
        "    }\n",
        "\n",
        "\n",
        "class POSTagger:\n",
        "    def __init__(self, word_embedding_dim=32, hidden_dim=64, batch_size=256, learning_rate=0.01, epochs=5, rnn_type='lstm', bidirectional=False):\n",
        "        self.word_embedding_dim = word_embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        if self.rnn_type not in ['lstm', 'gru']:\n",
        "            raise ValueError(\"rnn_type must be either 'lstm' or 'gru'\")\n",
        "\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "\n",
        "        self.token2idx = None\n",
        "        self.idx2token = None\n",
        "        self.tag2idx = None\n",
        "        self.idx2tag = None\n",
        "\n",
        "        self.model = None\n",
        "\n",
        "\n",
        "        self.train_dataset = None\n",
        "        self.test_dataset = None\n",
        "        self.train_loader = None\n",
        "        self.test_loader = None\n",
        "\n",
        "    def load_data(self, language=\"en_ewt\"):\n",
        "        print(f\"Loading dataset [universal_dependencies] with config = '{language}'\")\n",
        "        dataset = load_dataset(\"universal_dependencies\", language)\n",
        "\n",
        "        print(dataset)\n",
        "\n",
        "        for sentence in dataset['train']:\n",
        "            tokens = sentence[\"tokens\"]\n",
        "            tags = sentence[\"upos\"]\n",
        "            self.X.append(tokens)\n",
        "            self.y.append(tags)\n",
        "\n",
        "        assert len(self.X) == len(self.y)\n",
        "\n",
        "        print(self.X[0])\n",
        "        print(self.y[0])\n",
        "\n",
        "        sentences = [sentence for sentence in zip(self.X, self.y) if len(sentence[0]) > 2]\n",
        "        self.X = [s[0] for s in sentences]\n",
        "        self.y = [s[1] for s in sentences]\n",
        "\n",
        "        print(\"Loaded %i sentences\" % len(sentences))\n",
        "        print(sentences[0])\n",
        "\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=0.1, random_state=0\n",
        "        )\n",
        "\n",
        "        assert len(self.X_train) == len(self.y_train)\n",
        "        assert len(self.X_test) == len(self.y_test)\n",
        "        assert len(self.X_train) + len(self.X_test) == len(self.X)\n",
        "\n",
        "        print(\"The training set includes %i sentences\" % len(self.X_train))\n",
        "        print(\"The test set includes %i sentences\" % len(self.X_test))\n",
        "\n",
        "    def create_vocabulary(self):\n",
        "        tokens = set()\n",
        "        upos = set()\n",
        "\n",
        "        for sentence in self.X_train:\n",
        "            for token in sentence:\n",
        "                tokens.add(token)\n",
        "\n",
        "        for tags in self.y_train:\n",
        "            for tag in tags:\n",
        "                upos.add(tag)\n",
        "\n",
        "        self.idx2token = list(tokens)\n",
        "        self.idx2token.insert(0, '<UNK>')\n",
        "        self.idx2token.append('<PAD>')\n",
        "        self.token2idx = {token: idx for idx, token in enumerate(self.idx2token)}\n",
        "\n",
        "        self.idx2tag = list(upos)\n",
        "        self.idx2tag.append('<PAD>')\n",
        "        self.tag2idx = {tag: idx for idx, tag in enumerate(self.idx2tag)}\n",
        "\n",
        "        print(\"Token to Index Mapping:\", self.token2idx)\n",
        "        print(\"Index to Token Mapping:\", self.idx2token)\n",
        "        print(\"Tag to Index Mapping:\", self.tag2idx)\n",
        "        print(\"Index to Tag Mapping:\", self.idx2tag)\n",
        "\n",
        "    def setup_dataloaders(self):\n",
        "        self.train_dataset = POSDataset(\n",
        "            self.X_train,\n",
        "            self.y_train,\n",
        "            self.token2idx,\n",
        "            self.tag2idx\n",
        "        )\n",
        "\n",
        "        self.test_dataset = POSDataset(\n",
        "            self.X_test,\n",
        "            self.y_test,\n",
        "            self.token2idx,\n",
        "            self.tag2idx\n",
        "        )\n",
        "\n",
        "        self.train_loader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        self.test_loader = DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        print(f\"Created DataLoaders - Training batches: {len(self.train_loader)}, Test batches: {len(self.test_loader)}\")\n",
        "\n",
        "    def initialize_model(self):\n",
        "        self.model = RNNTagger(\n",
        "            word_embedding_dim=self.word_embedding_dim,\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            vocabulary_size=len(self.token2idx),\n",
        "            tagset_size=len(self.tag2idx)-1,\n",
        "            token2idx=self.token2idx,\n",
        "            tag2idx=self.tag2idx,\n",
        "            rnn_type=self.rnn_type,\n",
        "            bidirectional=self.bidirectional\n",
        "        )\n",
        "        print(self.model)\n",
        "\n",
        "    def train(self):\n",
        "        loss_function = nn.NLLLoss(ignore_index=self.tag2idx['<PAD>'])\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "            running_accuracy = 0.0\n",
        "            total_predictions = 0\n",
        "\n",
        "            progress_bar = tqdm(\n",
        "                self.train_loader,\n",
        "                desc=f\"Epoch {epoch+1}/{self.epochs} ({self.rnn_type.upper()}, {'Bi' if self.bidirectional else 'Uni'}directional)\",\n",
        "                unit=\"batch\"\n",
        "            )\n",
        "\n",
        "            for batch in progress_bar:\n",
        "                token_indices = batch['token_indices']\n",
        "                tag_indices = batch['tag_indices']\n",
        "                lengths = batch['lengths']\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    token_indices = token_indices.cuda()\n",
        "                    tag_indices = tag_indices.cuda()\n",
        "\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "\n",
        "                scores = self.model(token_indices, lengths)\n",
        "\n",
        "\n",
        "                loss = loss_function(\n",
        "                    scores.view(-1, self.model.tagset_size_),\n",
        "                    tag_indices.view(-1)\n",
        "                )\n",
        "\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "\n",
        "                predictions = scores.argmax(dim=2, keepdim=True).squeeze()\n",
        "                mask = tag_indices != self.tag2idx['<PAD>']\n",
        "                correct = (predictions[mask] == tag_indices[mask]).sum().item()\n",
        "                total = mask.sum().item()\n",
        "\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                running_accuracy += correct\n",
        "                total_predictions += total\n",
        "\n",
        "\n",
        "                batch_accuracy = 100 * correct / total if total > 0 else 0\n",
        "                progress_bar.set_postfix(\n",
        "                    loss=f\"{loss.item():.4f}\",\n",
        "                    accuracy=f\"{batch_accuracy:.2f}%\"\n",
        "                )\n",
        "\n",
        "\n",
        "                self.model.training_loss_.append(loss.item())\n",
        "                self.model.training_accuracy_.append(batch_accuracy)\n",
        "\n",
        "\n",
        "            epoch_loss = running_loss / len(self.train_loader)\n",
        "            epoch_accuracy = 100 * running_accuracy / total_predictions if total_predictions > 0 else 0\n",
        "            print(f\"Epoch {epoch+1}/{self.epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    def plot_training_progress(self):\n",
        "        fig = plt.figure(figsize=(6, 4))\n",
        "        ax = plt.subplot()\n",
        "        direction = 'Bidirectional' if self.bidirectional else 'Unidirectional'\n",
        "        ax.set_title(f\"Training Progress with {self.rnn_type.upper()} ({direction})\")\n",
        "        ax.plot(self.model.training_loss_, 'b-')\n",
        "        ax.set_ylabel(\"Training Loss\", color='b')\n",
        "        ax.set_xlabel(\"Batch\")\n",
        "        ax.tick_params(axis='y', labelcolor='b')\n",
        "        ax = ax.twinx()\n",
        "        ax.plot(self.model.training_accuracy_, 'r-')\n",
        "        ax.set_ylabel(\"Accuracy [%]\", color='r')\n",
        "        ax.tick_params(axis='y', labelcolor='r')\n",
        "        a = list(ax.axis())\n",
        "        a[2] = 0\n",
        "        a[3] = 100\n",
        "        ax.axis(a)\n",
        "        fig.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval()\n",
        "        n_correct = 0\n",
        "        n_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
        "\n",
        "                token_indices = batch['token_indices']\n",
        "                tag_indices = batch['tag_indices']\n",
        "                lengths = batch['lengths']\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    token_indices = token_indices.cuda()\n",
        "                    tag_indices = tag_indices.cuda()\n",
        "\n",
        "\n",
        "                scores = self.model(token_indices, lengths)\n",
        "\n",
        "\n",
        "                predictions = scores.argmax(dim=2, keepdim=True).squeeze()\n",
        "                mask = tag_indices != self.tag2idx['<PAD>']\n",
        "                n_correct += (predictions[mask] == tag_indices[mask]).sum().item()\n",
        "                n_total += mask.sum().item()\n",
        "\n",
        "        accuracy = 100 * n_correct / n_total if n_total > 0 else 0\n",
        "        direction = 'Bidirectional' if self.bidirectional else 'Unidirectional'\n",
        "        print(f\"Test accuracy with {self.rnn_type.upper()} ({direction}): {accuracy:.2f}%\")\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def run(self, language=\"en_ewt\"):\n",
        "        self.load_data(language=language)\n",
        "        self.create_vocabulary()\n",
        "        self.setup_dataloaders()\n",
        "        self.initialize_model()\n",
        "        self.train()\n",
        "        self.plot_training_progress()\n",
        "        self.evaluate()\n",
        "\n",
        "\n",
        "class RNNTagger(nn.Module):\n",
        "    def __init__(self, word_embedding_dim, hidden_dim, vocabulary_size, tagset_size, token2idx, tag2idx, rnn_type='lstm', bidirectional=False):\n",
        "        super(RNNTagger, self).__init__()\n",
        "        self.hidden_dim_ = hidden_dim\n",
        "        self.vocabulary_size_ = vocabulary_size\n",
        "        self.tagset_size_ = tagset_size\n",
        "        self.token2idx = token2idx\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self._word_embedding = nn.Embedding(\n",
        "            num_embeddings=vocabulary_size,\n",
        "            embedding_dim=word_embedding_dim,\n",
        "            padding_idx=token2idx['<PAD>']\n",
        "        )\n",
        "\n",
        "\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "\n",
        "        if self.rnn_type == 'lstm':\n",
        "            self._rnn = nn.LSTM(\n",
        "                input_size=word_embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                batch_first=True,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "        elif self.rnn_type == 'gru':\n",
        "            self._rnn = nn.GRU(\n",
        "                input_size=word_embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                batch_first=True,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "\n",
        "\n",
        "        rnn_output_dim = hidden_dim * self.num_directions\n",
        "\n",
        "        self._fc = nn.Linear(rnn_output_dim, tagset_size)\n",
        "        self._softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        self.training_loss_ = list()\n",
        "        self.training_accuracy_ = list()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "\n",
        "    def forward(self, padded_sentences, lengths):\n",
        "        \"\"\"The forward pass through the network\"\"\"\n",
        "        batch_size, max_sentence_length = padded_sentences.size()\n",
        "\n",
        "        embedded_sentences = self._word_embedding(padded_sentences)\n",
        "\n",
        "\n",
        "        X = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded_sentences,\n",
        "            lengths,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=True\n",
        "        )\n",
        "\n",
        "\n",
        "        rnn_out, _ = self._rnn(X)\n",
        "\n",
        "\n",
        "        X, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
        "\n",
        "        X = X.contiguous().view(-1, X.shape[2])\n",
        "        tag_space = self._fc(X)\n",
        "        tag_scores = self._softmax(tag_space)\n",
        "\n",
        "        return tag_scores.view(batch_size, max_sentence_length, self.tagset_size_)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rnn_type = 'lstm'\n",
        "    bidirectional = True\n",
        "\n",
        "    languages = [\"en_ewt\", \"es_gsd\", \"zh_gsdsimp\", \"de_gsd\"]\n",
        "    for lang in languages:\n",
        "        print(f\"=== Extension1: RNN={rnn_type}, Language={lang} ===\")\n",
        "        pos_tagger = POSTagger(\n",
        "            word_embedding_dim=32,\n",
        "            hidden_dim=64,\n",
        "            batch_size=256,\n",
        "            learning_rate=0.01,\n",
        "            epochs=5,\n",
        "            rnn_type=rnn_type\n",
        "        )\n",
        "        pos_tagger.run(language=lang)\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j7x5tqZkZzz"
      },
      "source": [
        "After using the PyTorch Dataset for data loading, the training curves and final accuracy for all four languages performed well: English and Spanish achieved over 97%, German reached 96%+, while Chinese was slightly lower at around 92%.  \n",
        "\n",
        "Overall, the training loss (blue line) decreased rapidly with the number of batches, and the accuracy (red line) continued to rise, indicating that the model was able to converge stably during mini-batch training. In comparison, English and Spanish exhibit higher morphological or syntactic predictability, making them easier for the model to learn. Chinese, on the other hand, lacks morphological markers and involves word segmentation ambiguities, leading to slightly lower accuracy. German falls between these two but still achieves strong results.  \n",
        "\n",
        "This experiment demonstrates that with a standardized Dataset/DataLoader processing pipeline, the model can still achieve high accuracy in multilingual part-of-speech tagging tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thV2yDkdgJqK"
      },
      "source": [
        "### Model Complexity Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F9jbJThNgPDN",
        "outputId": "06160433-c363-4fd1-b181-216828634bbc"
      },
      "outputs": [],
      "source": [
        "model_complexities = [\n",
        "    {\"name\": \"LowComplex\", \"embedding_dim\": 32, \"hidden_dim\": 64},\n",
        "    {\"name\": \"HighComplex\", \"embedding_dim\": 64, \"hidden_dim\": 128},\n",
        "]\n",
        "\n",
        "languages = [\"en_ewt\", \"es_gsd\", \"zh_gsdsimp\", \"de_gsd\"]\n",
        "\n",
        "for mc in model_complexities:\n",
        "    print(f\"=== Model Complexity: {mc['name']} (emb={mc['embedding_dim']}, hid={mc['hidden_dim']}) ===\")\n",
        "    for lang in [\"en_ewt\", \"es_gsd\", \"zh_gsdsimp\", \"de_gsd\"]:\n",
        "        print(f\"--- Language: {lang} ---\")\n",
        "\n",
        "        tagger = POSTagger(\n",
        "            word_embedding_dim=mc[\"embedding_dim\"],\n",
        "            hidden_dim=mc[\"hidden_dim\"],\n",
        "            batch_size=256,\n",
        "            learning_rate=0.01,\n",
        "            epochs=5,\n",
        "            rnn_type='lstm'\n",
        "        )\n",
        "        tagger.run(language=lang)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od75RkB2jcYG"
      },
      "source": [
        "Under the low-complexity model, the accuracy for the four languages mostly falls between 90% and 97%. With the high-complexity model, accuracy generally improves further, with some languages reaching around 98% to 99%.  \n",
        "\n",
        "Overall, increasing the model dimensions allows for better capturing of morphological, word order, and contextual features across different languages, leading to faster convergence or higher final accuracy. However, for some languages, the improvement is relatively limited, suggesting that under this task and dataset size, a smaller network is already capable of learning the key patterns. On the other hand, while larger models tend to reach high accuracy more quickly in the early training stages, it is also important to consider the potential risk of overfitting."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "nlp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
